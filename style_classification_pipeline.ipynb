{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "import codecs\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open(u'stopwords.txt','r','utf8') as f:\n",
    "   stopwords=set([line.strip() for line in f])\n",
    "   \n",
    "raw_data=pd.read_excel('data_by_grade/middle_essay.xlsx')\n",
    "\n",
    "target=raw_data.styles\n",
    "data=raw_data.answer_clean.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "写景     1700\n",
       "状物     1700\n",
       "写人     1700\n",
       "散文     1700\n",
       "叙事     1700\n",
       "议论文    1700\n",
       "Name: styles, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to scikit-learn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text),\n",
    "                 'num_sentences': text.count(u'。')}\n",
    "                for text in posts]\n",
    "\n",
    "class TokenPosGenerator(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        features = np.recarray(shape=(len(posts),),\n",
    "                               dtype=[('Tokened', object), ('Pos', object),('raw_text',object)])\n",
    "        for i, text in enumerate(posts):\n",
    "            pos_list=[]\n",
    "            word_list=[]\n",
    "            for w,p in pseg.cut(text):\n",
    "                word_list.append(w)\n",
    "                pos_list.append(p)\n",
    "\n",
    "            features['Tokened'][i] = ' '.join(word_list)\n",
    "            features['Pos'][i] = pos_list\n",
    "            features['raw_text'][i] = text\n",
    "        return features\n",
    "\n",
    "\n",
    "class PosTranVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, pos_list=None):\n",
    "        if pos_list is not None:\n",
    "            self.pos_list = pos_list\n",
    "        else:\n",
    "            self.pos_list = [u'a', u'ad', u'ag', u'an', u'b', u'c', u'd', u'df', u'dg', u'e', \n",
    "                             u'eng', u'f', u'g', u'h', u'i', u'j', u'k', u'l', u'm', u'mg', \n",
    "                             u'mq', u'n', u'ng', u'nr', u'nrfg', u'nrt', u'ns', u'nt', u'nz', \n",
    "                             u'o', u'p', u'q', u'r', u'rg', u'rr', u'rz', u's', u't', u'tg', \n",
    "                             u'u', u'ud', u'ug', u'uj', u'ul', u'uv', u'uz', u'v', u'vd', u'vg', \n",
    "                             u'vi', u'vn', u'vq', u'x', u'y', 'yg', u'z', u'zg']\n",
    "\n",
    "        self.pos2id = dict((w, i) for i, w in enumerate(self.pos_list))\n",
    "\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, posts):\n",
    "        N = len(self.pos2id)\n",
    "        self.pos_features_name = ['_'.join(w) for w in zip(np.repeat(self.pos_list, len(self.pos_list)).tolist(),\n",
    "                                                           self.pos_list * len(self.pos_list))]\n",
    "        pos_features = np.empty((len(posts),N**2),dtype=float)\n",
    "        for i, poslst in enumerate(posts):\n",
    "            pos_features[i] = self._pos_transition(poslst, N)\n",
    "        return pos_features\n",
    "    \n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.pos_features_name\n",
    "    \n",
    "    \n",
    "    def _pos_transition(self, pos, N):\n",
    "        mat = np.zeros((N, N), dtype=float)\n",
    "        for i in range(len(pos) - 1):\n",
    "            index1 = self.pos2id.get(pos[i], None)\n",
    "            index2 = self.pos2id.get(pos[i + 1], None)\n",
    "            if index1 is not None and index2 is not None:\n",
    "                mat[index1, index2] = + 1\n",
    "        return (mat / np.sum(mat)).reshape((1, N ** 2))\n",
    "\n",
    "#pos_stater=PosTranVectorizer()\n",
    "#pos_stater.transform([poslst])\n",
    "#pos_stater.get_feature_names()\n",
    "\n",
    "class PosStatVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, pos_list=None):\n",
    "        if pos_list is not None:\n",
    "            self.pos_list = pos_list\n",
    "        else:\n",
    "            self.pos_list = [u'a', u'ad', u'ag', u'an', u'b', u'c', u'd', u'df', u'dg', u'e', \n",
    "                             u'eng', u'f', u'g', u'h', u'i', u'j', u'k', u'l', u'm', u'mg', \n",
    "                             u'mq', u'n', u'ng', u'nr', u'nrfg', u'nrt', u'ns', u'nt', u'nz', \n",
    "                             u'o', u'p', u'q', u'r', u'rg', u'rr', u'rz', u's', u't', u'tg', \n",
    "                             u'u', u'ud', u'ug', u'uj', u'ul', u'uv', u'uz', u'v', u'vd', u'vg', \n",
    "                             u'vi', u'vn', u'vq', u'x', u'y', 'yg', u'z', u'zg']\n",
    "        self.pos2id = dict((w, i) for i, w in enumerate(self.pos_list))\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, posts):\n",
    "        N = len(self.pos2id)\n",
    "        pos_features = np.zeros((len(posts),N),dtype=float)\n",
    "        for i, poslst in enumerate(posts):\n",
    "            for p in poslst:\n",
    "                pos_features[i,self.pos2id[p]] += 1\n",
    "                pos_features[i] /= pos_features[i].sum()  \n",
    "        return pos_features\n",
    "        \n",
    "    def get_feature_names(self):\n",
    "        return self.pos_list\n",
    "\n",
    "class Sparse2Mat(BaseEstimator, TransformerMixin): \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,sparseMatrix):\n",
    "        return sparseMatrix.toarray()\n",
    "\n",
    "#pos_stater=PosStatVectorizer()\n",
    "#pos_stater.transform([poslst])\n",
    "#pos_stater.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_union_pipeline = Pipeline([\n",
    "    #preprocess\n",
    "    ('textToken', TokenPosGenerator()),\n",
    "    #feature union\n",
    "    ('union', FeatureUnion(\n",
    "        n_jobs=4,\n",
    "        transformer_list = [\n",
    "            #extract feature from raw_text\n",
    "            ('text_stat',Pipeline([\n",
    "                ('selector', ItemSelector(key = 'raw_text')),\n",
    "                ('stats', TextStats()),\n",
    "                ('vect', DictVectorizer())\n",
    "            ])),\n",
    "            #extract feature from Tokened\n",
    "            ('token_tfidf',Pipeline([\n",
    "                ('selector', ItemSelector(key = 'Tokened')),\n",
    "                ('tfidf',TfidfVectorizer(max_df=0.90,min_df=5,max_features=1000,stop_words=stopwords))\n",
    "            ])),\n",
    "            #extract feature from Pos\n",
    "            ('pos_tran',Pipeline([\n",
    "                ('selector', ItemSelector(key = 'Pos')),\n",
    "                ('pos', PosTranVectorizer())\n",
    "            ])),\n",
    "            ('pos_stat',Pipeline([\n",
    "                ('selector', ItemSelector(key = 'Pos')),\n",
    "                ('pos', PosStatVectorizer())\n",
    "            ])),\n",
    "            \n",
    "        ]\n",
    "    )),\n",
    "    #convert a sparse matrix to array\n",
    "    ('converter',Sparse2Mat()),\n",
    "    #normalizatin\n",
    "    ('scaler', StandardScaler()),\n",
    "    #feature selection\n",
    "    \n",
    "    #classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_data=feature_union_pipeline.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unioner=feature_union_pipeline.named_steps['union']\n",
    "F1 = unioner.transformer_list[0][1].named_steps['vect'].get_feature_names()\n",
    "F2 = unioner.transformer_list[1][1].named_steps['tfidf'].get_feature_names()\n",
    "F3 = unioner.transformer_list[2][1].named_steps['pos'].get_feature_names()\n",
    "feature_names = F1+F2+F3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_pipeline = Pipeline([\n",
    "    ('selector', SelectFromModel(LinearSVC(C=0.01, penalty=\"l1\", dual=False))),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=500,oob_score=True))\n",
    "]) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_data, target,  \n",
    "                                                    train_size=0.75,  \n",
    "                                                    test_size=0.25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_pipeline.fit(X_train,y_train) \n",
    "rf_ret_proba=rf_pipeline.predict_proba(X_test)  \n",
    "rf_ret=rf_pipeline.predict(X_test)     \n",
    "             \n",
    "metrics.accuracy_score(y_test, rf_ret)\n",
    "\n",
    "rf_model=rf_pipeline.named_steps['classifier']\n",
    "selet_model=rf_pipeline.named_steps['selector']  \n",
    "selet_model.get_support()#保留的特征\n",
    "\n",
    "conf_matrix=pd.DataFrame(metrics.confusion_matrix(y_test, rf_ret),index=rf_model.classes_,columns=rf_model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57725490196078433"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, rf_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>写人</th>\n",
       "      <th>写景</th>\n",
       "      <th>叙事</th>\n",
       "      <th>散文</th>\n",
       "      <th>状物</th>\n",
       "      <th>议论文</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>写人</th>\n",
       "      <td>270</td>\n",
       "      <td>12</td>\n",
       "      <td>67</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>写景</th>\n",
       "      <td>4</td>\n",
       "      <td>344</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>叙事</th>\n",
       "      <td>94</td>\n",
       "      <td>34</td>\n",
       "      <td>198</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>散文</th>\n",
       "      <td>39</td>\n",
       "      <td>92</td>\n",
       "      <td>43</td>\n",
       "      <td>104</td>\n",
       "      <td>38</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>状物</th>\n",
       "      <td>21</td>\n",
       "      <td>109</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>220</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>议论文</th>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      写人   写景   叙事   散文   状物  议论文\n",
       "写人   270   12   67   24   24   54\n",
       "写景     4  344    8   16   28    7\n",
       "叙事    94   34  198   23   18   46\n",
       "散文    39   92   43  104   38  117\n",
       "状物    21  109   17   19  220   37\n",
       "议论文   24   13   25   18    7  336"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2678"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selet_model.get_support().sum()#保留的特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初中pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "middle_style_classification= Pipeline([\n",
    "    #preprocess\n",
    "    ('textToken', TokenPosGenerator()),\n",
    "    #feature union\n",
    "    ('union', FeatureUnion(\n",
    "        n_jobs=4,\n",
    "        transformer_list = [\n",
    "            #extract feature from raw_text\n",
    "            ('text_stat',Pipeline([\n",
    "                ('selector', ItemSelector(key = 'raw_text')),\n",
    "                ('stats', TextStats()),\n",
    "                ('vect', DictVectorizer())\n",
    "            ])),\n",
    "            #extract feature from Tokened\n",
    "            ('token_tfidf',Pipeline([\n",
    "                ('selector', ItemSelector(key = 'Tokened')),\n",
    "                ('tfidf',TfidfVectorizer(max_df=0.90,min_df=5,max_features=1000,stop_words=stopwords))\n",
    "            ])),\n",
    "            #extract feature from Pos\n",
    "            ('pos_tran',Pipeline([\n",
    "                ('selector', ItemSelector(key = 'Pos')),\n",
    "                ('pos', PosTranVectorizer())\n",
    "            ])),\n",
    "            ('pos_stat',Pipeline([\n",
    "                ('selector', ItemSelector(key = 'Pos')),\n",
    "                ('pos', PosStatVectorizer())\n",
    "            ])),\n",
    "            \n",
    "        ]\n",
    "    )),\n",
    "    #convert a sparse matrix to array\n",
    "    ('converter',Sparse2Mat()),\n",
    "    #normalizatin\n",
    "    ('scaler', StandardScaler()),\n",
    "    #feature selection\n",
    "    ('selector', SelectFromModel(LinearSVC(C=0.01, penalty=\"l1\", dual=False))),\n",
    "    #classifier\n",
    "    ('classifier', RandomForestClassifier(n_estimators=500,oob_score=True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('textToken', TokenPosGenerator()), ('union', FeatureUnion(n_jobs=4,\n",
       "       transformer_list=[('text_stat', Pipeline(steps=[('selector', ItemSelector(key='raw_text')), ('stats', TextStats()), ('vect', DictVectorizer(dtype=<type 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=True))...imators=500, n_jobs=1,\n",
       "            oob_score=True, random_state=None, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "middle_style_classification.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2927"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model=middle_style_classification.named_steps['classifier']\n",
    "selet_model=middle_style_classification.named_steps['selector']  \n",
    "selet_model.get_support().sum()#保留的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     写人\n",
       "1     写景\n",
       "2     叙事\n",
       "3     散文\n",
       "4     状物\n",
       "5    议论文\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(rf_model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.758,  0.014,  0.088,  0.04 ,  0.07 ,  0.03 ],\n",
       "       [ 0.684,  0.01 ,  0.11 ,  0.076,  0.056,  0.064],\n",
       "       [ 0.752,  0.016,  0.094,  0.036,  0.042,  0.06 ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "middle_style_classification.predict_proba(data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['styleModel/middle_style_classification.model']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(middle_style_classification,'styleModel/middle_style_classification.model',compress=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
